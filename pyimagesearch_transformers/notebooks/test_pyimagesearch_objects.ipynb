{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c677f03c-5bda-4a8e-a4aa-acd24971a0c1",
   "metadata": {},
   "source": [
    "# Test `pyimagesearch` object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0074cf4-591c-4964-ae7f-ba20367f7dab",
   "metadata": {},
   "source": [
    "Objectives: play around with and test the objects defined in the modules within the `pyimagesearch/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9d4425-4e58-46dd-bf00-bbe0b89db796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sys.path.append('../pyimagesearch/')\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b26efe9-8255-4607-a80b-8395669a7315",
   "metadata": {},
   "source": [
    "## `dataset.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83980c41-7fdf-427d-a3f4-80c6d3ab1536",
   "metadata": {},
   "source": [
    "**Note:** make sure to download (and point to) the correct file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82142dd8-e801-4571-899a-e0c9999d2a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "from dataset import load_data, splitting_dataset, make_dataset, tf_lower_and_split_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14f5801-6888-4da1-a267-0f8ddc2e5dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = '../data/fra.txt'\n",
    "\n",
    "source, target = load_data(fname)\n",
    "\n",
    "print('Some source sentences:', source[:3])\n",
    "print('Some target sentences:', target[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf33ae4-adb4-4044-ab7d-d4facf0ed5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    (trainSource, trainTarget),\n",
    "    (valSource, valTarget),\n",
    "    (testSource, testTarget)\n",
    ") = splitting_dataset(source, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2d7061-2166-43c5-85bc-63e8127f5514",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = make_dataset(\n",
    "    splits=(trainSource, trainTarget),\n",
    "    batchSize=16,\n",
    "    # The text processors should probably be instances\n",
    "    # with particular initializations (depending on the\n",
    "    # source and target vocabularies etc.). Here it's\n",
    "    # sufficient the function runs correctly.\n",
    "    sourceTextProcessor=TextVectorization(),\n",
    "    targetTextProcessor=TextVectorization(),\n",
    "    train=False\n",
    ")\n",
    "\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35b8dc4-9c1a-4325-9e08-d812bd74e5a9",
   "metadata": {},
   "source": [
    "Work on a test sentences with the text manipulation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbbe881-472c-4518-8f53-e3e40e003b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    \"Six o'clock on the Christmas morning...\",\n",
    "    \"and for what?\",\n",
    "    \"Six o'clock the siren kicks him from a dream\",\n",
    "    \"Tries to shake it off but it just won't stop\"\n",
    "]\n",
    "\n",
    "processed_test_sentences = tf_lower_and_split_punct(test_sentences)\n",
    "\n",
    "processed_test_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a995e8-881a-450d-b3c4-79cdecf901ce",
   "metadata": {},
   "source": [
    "## `attention.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee593aea-43c8-4bf7-9721-7bc0127ca3c7",
   "metadata": {},
   "source": [
    "From the Keras documentation: layers (and any subclass) are **callable objects** in which operations are implemented in their **call** method. So:\n",
    "- To act with a layer on some input, just pass the input to it as you would to a function.\n",
    "- To define what the layer does, look at its **call** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cf8621-2d7c-4c22-8588-52e17254d9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from attention import BaseAttention, CrossAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0151027b-676c-427f-9808-44cd74495a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = tf.random.uniform(shape=(10, 100))\n",
    "context_test = tf.random.uniform(shape=(50, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7d6012-6878-41b7-a02e-9bdd611ebfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bal = BaseAttention(num_heads=4, key_dim=10)\n",
    "\n",
    "bal(tf.concat([x_test, context_test], axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20221c9e-c7c1-475a-896f-3bd51d16446c",
   "metadata": {},
   "source": [
    "## `positional_encoding.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fffa2ae-42e2-4195-b4a0-77096cabd76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from positional_encoding import positional_encoding, PositionalEmbedding\n",
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f364ffde-7c93-48c8-bc33-8d2ca56b8849",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lenght = 50  # Max length of a sequence.\n",
    "test_depth = 50  # Length of the representation of each position.\n",
    "\n",
    "pe = positional_encoding(length=test_lenght, depth=test_depth)\n",
    "\n",
    "pe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fdd1ab-2d7b-40d6-ad2a-8893553d0f4d",
   "metadata": {},
   "source": [
    "Transform the processed test sentences (still in natural language) into a numerical tensor via a `TextVectorization` layer, then get its embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1c2c82-5133-461d-beb4-e69fea2037bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb570ef4-174f-4fbe-8adb-ae2e3fc599db",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vocabulary = np.unique(\n",
    "    list(chain.from_iterable([s.numpy().decode('utf-8').split(' ') for s in processed_test_sentences]))\n",
    ")\n",
    "\n",
    "tvl = TextVectorization(\n",
    "    # We extract all the unique tokens from the processed sentences and\n",
    "    # use them as the vocabulary.\n",
    "    vocabulary=test_vocabulary\n",
    ")\n",
    "\n",
    "vectorized_sentences = tvl(processed_test_sentences)\n",
    "\n",
    "vectorized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc96ea3-688d-4dd2-a5d6-2aaadfb92824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with the original processed sentences in natural\n",
    "# language.\n",
    "processed_test_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ca91cb-b542-4842-8d28-3f0e30f339e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dModel = 50\n",
    "\n",
    "test_vocab_size = test_vocabulary.shape[0] + 2\n",
    "test_max_pos_encoding = 50\n",
    "\n",
    "pos_emb = PositionalEmbedding(\n",
    "    # We must add at least 2 to the vocebulary size for this\n",
    "    # to work, probably because of special tokens like\n",
    "    # padding with zeros or start/end sentence tokens.\n",
    "    vocabSize=test_vocab_size,\n",
    "    dModel=dModel,\n",
    "    maximumPositionEncoding=test_max_pos_encoding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fd87e8-579b-45d6-829c-91ef09c303dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.imshow(pos_emb.posEncoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e38b3ec-e482-44f2-96b6-19b4fb7f8772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the embedding with positional encoding information for\n",
    "# the vectorized sentences.\n",
    "test_embeddings = pos_emb(vectorized_sentences)\n",
    "\n",
    "test_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a150a53f-16da-42e8-9832-1a8d49020db8",
   "metadata": {},
   "source": [
    "## `feedforward.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec0165b-8706-41bc-a544-cf0d5d170c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feed_forward import FeedForward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97e0726-1ce4-449e-bc19-77b02cf66710",
   "metadata": {},
   "source": [
    "Process the test embeddings with the custom `FeedForward` layer. Notice how the input and output shape do not change: that's needed for the skip connection (adding up the input tensor back to the output one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd96e37e-647a-405c-a9e8-4fce0ff99528",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = FeedForward(\n",
    "    dff=128,\n",
    "    dModel=dModel\n",
    ")\n",
    "\n",
    "print(f'Test embeddings shape: {test_embeddings.shape}')\n",
    "\n",
    "ff(test_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2131e0-1dd4-487d-aa33-0d3ced643416",
   "metadata": {},
   "source": [
    "## `rate_schedule.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33a63d5-d557-45b1-b84e-0157f7d7adb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rate_schedule import CustomSchedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de67184e-ec11-49d7-91ff-713cff54f887",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = CustomSchedule(dModel=dModel, warmupSteps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8ef374-ea0d-476b-8620-1a851b888bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = tf.range(1, 100)\n",
    "\n",
    "schedule = cs(steps)\n",
    "\n",
    "schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6836730d-e8e1-4059-af8a-40799bc7e4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "sns.lineplot(\n",
    "    x=steps,\n",
    "    y=schedule\n",
    ")\n",
    "\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Learning rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8527a257-cad5-4927-bf30-39df010bc722",
   "metadata": {},
   "source": [
    "## `loss_accuracy.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712a3e40-fb0e-44a4-a95d-4e4875e7f1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from loss_accuracy import masked_loss, masked_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c55141-f281-4569-b94d-62d61d0ec6f6",
   "metadata": {},
   "source": [
    "Generate fake labels and predictions and compute the masked loss and accuracy over them.\n",
    "\n",
    "**Note:** shapes and number of labels are chosen so that the loss and accuracy are computed correctly. The requirement is that since the predictions are logits, each prediction is a tensor with a shape equal to the number of labels. Also, \"middle\" shape of the predictions tensor has been chosen so as to make both the loss and the accuracy work (the accuracy looks for axis 2 when computing the argmax)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31209f17-1858-4eb1-b3d4-7fbec909e727",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 50\n",
    "\n",
    "test_labels = tf.random.uniform(shape=[n_samples, 1], minval=0, maxval=10, dtype=tf.int32)\n",
    "\n",
    "test_predictions = tf.random.uniform(shape=[n_samples, test_vocabulary.shape[0] + 2])\n",
    "test_predictions = test_predictions / tf.reduce_sum(test_predictions, axis=-1)[..., tf.newaxis]\n",
    "test_predictions = test_predictions[:, tf.newaxis, :]\n",
    "\n",
    "print('Masked loss:', masked_loss(test_labels, test_predictions))\n",
    "print('Masked accuracy:', masked_accuracy(test_labels, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b442a34d-4337-4014-bc68-094ebc6d279a",
   "metadata": {},
   "source": [
    "## `encoder.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8623fd-b539-45cd-8364-a518a04e2108",
   "metadata": {},
   "outputs": [],
   "source": [
    "from encoder import EncoderLayer, Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7287ad-5128-481e-9494-e85478cf161a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(\n",
    "    numLayers=4,\n",
    "    dModel=dModel,\n",
    "    numHeads=4,\n",
    "    sourceVocabSize=test_vocab_size,\n",
    "    maximumPositionEncoding=test_max_pos_encoding,\n",
    "    dff=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a3f7c2-6144-41d8-8542-73a3d1b8c087",
   "metadata": {},
   "source": [
    "Passing the vectorized sentences to the encoder (with the correct dimensions found before) should work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7f1077-a3ca-481e-8a5d-c7245f74f452",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder(vectorized_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e80315-e039-4e53-86fc-b8f2a5a6bfa6",
   "metadata": {},
   "source": [
    "## `decoder.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2611e80-3dd5-4a78-9ff5-ec3a6c458655",
   "metadata": {},
   "outputs": [],
   "source": [
    "from decoder import DecoderLayer, Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad86d09c-86a3-41d0-b808-fc7a11eb88d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_test_sentences = [\n",
    "    \"Sei del mattino la mattina di Natale...\",\n",
    "    \"e per cosa?\",\n",
    "    \"Sei del mattino la sirena lo sveglia da un sogno\",\n",
    "    \"Cerca di scrollarselo via ma proprio non la smette\"\n",
    "]\n",
    "\n",
    "processed_target_test_sentences = tf_lower_and_split_punct(target_test_sentences)\n",
    "\n",
    "processed_target_test_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f999dd-858d-4781-a38c-fb577254e86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_test_vocabulary = np.unique(\n",
    "    list(chain.from_iterable([s.numpy().decode('utf-8').split(' ') for s in processed_target_test_sentences]))\n",
    ")\n",
    "\n",
    "tvl_target = TextVectorization(\n",
    "    # We extract all the unique tokens from the processed sentences and\n",
    "    # use them as the vocabulary.\n",
    "    vocabulary=target_test_vocabulary\n",
    ")\n",
    "\n",
    "vectorized_target_sentences = tvl_target(processed_target_test_sentences)\n",
    "\n",
    "vectorized_target_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68721b1-468a-412c-9319-fd0e744887e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_test_vocab_size = target_test_vocabulary.shape[0] + 2\n",
    "\n",
    "decoder = Decoder(\n",
    "    numLayers=4,\n",
    "    dModel=dModel,\n",
    "    numHeads=4,\n",
    "    targetVocabSize=target_test_vocab_size,\n",
    "    maximumPositionEncoding=test_max_pos_encoding,\n",
    "    dff=512\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19715105-1b30-4be8-9065-99a160bc9254",
   "metadata": {},
   "source": [
    "## `transformer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23f90dc-8a02-4964-9945-879c39957cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffc5221-0fe6-4803-9fae-e7c05d0a3d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "trns = Transformer(\n",
    "    encNumLayers=4,\n",
    "    decNumLayers=4,\n",
    "    dModel=dModel,\n",
    "    numHeads=4,\n",
    "    dff=512,\n",
    "    sourceVocabSize=test_vocab_size,\n",
    "    targetVocabSize=target_test_vocab_size,\n",
    "    maximumPositionEncoding=test_max_pos_encoding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25987545-7817-4b45-8649-9da00b00b376",
   "metadata": {},
   "outputs": [],
   "source": [
    "trns(inputs=(vectorized_sentences, vectorized_target_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9f2dda-228e-4efb-a23f-733a36777879",
   "metadata": {},
   "source": [
    "## `translator.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16b25f4-caa0-472f-b6e6-3bc599c3f6b2",
   "metadata": {},
   "source": [
    "Test the the `Translator` object is at least initialized without errors: for the actual translation more details must be known from the various objects that come with it (e.g. the text processors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a1ca34-2752-4894-b20f-3d9330713694",
   "metadata": {},
   "outputs": [],
   "source": [
    "from translate import Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8976c92-03c3-4d75-aca2-470223d90d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_translator = Translator(\n",
    "    sourceTextProcessor=tvl,\n",
    "    targetTextProcessor=tvl_target,\n",
    "    transformer=trns,\n",
    "    maxLength=20\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
