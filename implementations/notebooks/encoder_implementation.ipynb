{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c64cf3d-7244-4b01-8200-8f4d9b3ae54e",
   "metadata": {},
   "source": [
    "# Implementing a full transformer encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a38f829-8406-499f-b17a-4f68bc73eaf9",
   "metadata": {},
   "source": [
    "__Objective:__ implement the encoder part of a transformer model from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be906773-3924-4b3c-85da-09a14657e85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "sys.path.append('../modules/')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a552ea6-646a-4bca-ad17-6d14ddcd29bd",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9838d82-6772-44da-be3c-4e140eda5fc6",
   "metadata": {},
   "source": [
    "An attention layer works with __token embeddings__ as the input, so we need to start by tokenizing the input text and creating the vector embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801f3776-81cc-4861-bd87-225fa2f8f76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a tokenizer from a model.\n",
    "model_ckpt = 'distilbert-base-uncased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f245ef09-319f-4fd0-8616-09e36ab77003",
   "metadata": {},
   "source": [
    "Test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21573b4e-7634-44fd-abc7-728a691e2199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the tokenizer.\n",
    "test_text = \"\"\"\n",
    "I know all about the honor of God, Mary Jane.\n",
    "\"\"\"\n",
    "\n",
    "test_output = tokenizer(\n",
    "    test_text,\n",
    "    return_tensors='tf',\n",
    "    padding=True,\n",
    "    # In this case we exclude the start- and end-of-sentence tokens.\n",
    "    add_special_tokens=False\n",
    ")\n",
    "\n",
    "test_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6b4b78-4cca-46b4-b2d5-6b4af99fa7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output['input_ids'].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c118a5-bb29-4138-a5e2-555909f804bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(test_output['input_ids'][0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f81aa5-e425-4246-96d3-d590b791e3e8",
   "metadata": {},
   "source": [
    "## Creation of embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254112bc-b95e-4708-8305-071d8cde6fe5",
   "metadata": {},
   "source": [
    "Create the word embeddings (vectors) from the tokenized text.\n",
    "\n",
    "Keras' `Embedding` layer maps positive integers (tokenized text) to dense vectors of fixed size.\n",
    "\n",
    "__Notes:__\n",
    "- At this point the embeddings of the tokens know nothing about the context - each token's embedding is always the same, __irrespective of the context__ (i.e. the embedding operation is deterministic). The attention layer is there right to modify the embeddings to include context-depending information.\n",
    "- We skip positional encoding for simplicity, but the information thereof should be added to the token embeddings at this point!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d30bfc5-03a0-4554-9423-8156758bef50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the configuration parameters of the pretrained model.\n",
    "config = AutoConfig.from_pretrained(model_ckpt)\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e996b98-e996-44eb-8aa6-1919fa48c475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the embedding layer.\n",
    "token_emb = Embedding(\n",
    "    input_dim=config.vocab_size,  # We could have used tokenizer.vocab_size, it's the same.\n",
    "    output_dim=config.hidden_size\n",
    ")\n",
    "\n",
    "token_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b423ce-ca2a-4bc0-8138-8b69b6070ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the creation of embedding for some tokenized text.\n",
    "# Output shape: (batch_size, seq_len, hidden_dim).\n",
    "test_embeddings = token_emb(test_output['input_ids'])\n",
    "\n",
    "test_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0927a2-59a7-4b4d-8af2-d65f1bb06173",
   "metadata": {},
   "source": [
    "### Add positional encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b92e3f-00b4-46f3-9cc7-35de331dc484",
   "metadata": {},
   "source": [
    "We now add positional encoding to the embeddings, so each embedding also contains information of the position of the corresponding token in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f128a2-7a99-4f59-85c7-4eb0dcdaa437",
   "metadata": {},
   "outputs": [],
   "source": [
    "from encoder import Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04312b7-45ec-4790-8689-beeeb615332e",
   "metadata": {},
   "source": [
    "Test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d38397-7898-41f1-a3a5-1b352a032233",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\n",
    "    \"Six o' clock on the Christmas morning...\",\n",
    "    \"...and for what?\"\n",
    "]\n",
    "\n",
    "test_token_ids = tokenizer(\n",
    "    text,\n",
    "    return_tensors='tf',\n",
    "    padding=True,\n",
    "    add_special_tokens=True\n",
    ")['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f7d40d-9eac-4254-b113-2e62ec3c0c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embeddings(config=config)\n",
    "\n",
    "test_embeddings = embedding_layer(test_token_ids)\n",
    "\n",
    "test_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902b79a9-e55f-4079-a94c-bfbded33117c",
   "metadata": {},
   "source": [
    "## A basic self-attention mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f6e212-bc48-4d9f-9526-647ea1fe4f8c",
   "metadata": {},
   "source": [
    "We reproduce the basic operations for a single-head attention layer, acting on the test embeddings obtained above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8194876-c05c-4753-964a-09be56832a2b",
   "metadata": {},
   "source": [
    "### Creation of query, key and value vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f0961c-c17e-4487-a0c7-51abcdc739a4",
   "metadata": {},
   "source": [
    "For simplicity, we can take the query, key and value vectors associated to each token embedding equal to the token embedding itself (and thus also equal to one another). This need not be the case: in general, independent weight matrices (__trainable__) are applied to get the query, key and value vectors from the token embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ed8c39-4c04-4d8d-88ed-d1a74d01538f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = test_embeddings\n",
    "key = test_embeddings\n",
    "value = test_embeddings\n",
    "\n",
    "dim_k = key.shape[-1]\n",
    "\n",
    "dim_k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173cbbb4-0391-4f04-86c5-87e245d6fb51",
   "metadata": {},
   "source": [
    "### Attention scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696a0086-0577-4b6a-842c-6df6c714ab01",
   "metadata": {},
   "source": [
    "Given an input, the attention scores (not the weights yet!) are computed as the dot product of each query vector with each key vector. This measures the similarity (relevance) of each key w.r.t. each query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10105526-9b6e-4604-8d3e-135fb74fa001",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.shape, key.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85db8d14-8ed6-496e-82b7-208bdacdf1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output shape: (batch_size, seq_len, seq_len).\n",
    "scores = tf.matmul(\n",
    "    query,\n",
    "    # Leaving the batch shape as the first dimension, it's ignored\n",
    "    # in the matrix multiplication.\n",
    "    tf.transpose(key, perm=(0, 2, 1))\n",
    ")\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb52d37d-71d2-4253-8e22-8e137f85598a",
   "metadata": {},
   "source": [
    "### Attention weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90649227-9b90-428f-b90b-164034754198",
   "metadata": {},
   "source": [
    "Attention weights are obtained from attention scores by:\n",
    "1. Rescaling the scores dividing by $\\sqrt{\\text{hidden dim}}$. This is done to avoid too large scores, which would mess up with the gradient descent steps in the training phase.\n",
    "2. Applying the `softmax` function to the last axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d85f777-7831-44ab-b40a-37491d9e9db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = tf.math.softmax(\n",
    "    scores / tf.sqrt(tf.cast(dim_k, tf.float32)),\n",
    "    axis=-1\n",
    ")\n",
    "\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ee187c-7829-41d9-bc83-45d0c29e1186",
   "metadata": {},
   "source": [
    "Check: row by row, if we add up all the entries in the columns we should get a value close to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a136fe-19ff-4a24-84aa-b2a4ec8eebb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_sum(weights, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007206c7-b2be-4f1a-a62e-af18a30c4030",
   "metadata": {},
   "source": [
    "### Output of the self-attention layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b838e8-42b1-4a6a-bce9-c620a945bb66",
   "metadata": {},
   "source": [
    "The output of the layer is a linear combination of the value vectors with weights given by the attention weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8c4000-cbf3-4c12-b839-bab075a44de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output shape: (batch_size, seq_len, value_size).\n",
    "test_attention_output = tf.matmul(weights, value)\n",
    "\n",
    "test_attention_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04294934-21cc-4317-b5de-72968b82440d",
   "metadata": {},
   "source": [
    "## Multi-headed attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a0e578-2f9a-4b70-be3f-0ef0ee2e6fa0",
   "metadata": {},
   "source": [
    "Implement a multi-head attention layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6034011e-ba0e-49c5-9443-9b7ed7e287a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import scaled_dot_product_attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6e8eb3-eb0e-4993-a378-f1a1dc668348",
   "metadata": {},
   "source": [
    "Define a single attention head layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5be73b6-4dd9-49d1-8859-9458d1cc234a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from encoder import AttentionHead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bde0fc-ccc0-4e7d-8063-453e64d52b31",
   "metadata": {},
   "source": [
    "Test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7109f1b-7cd4-43b1-86f4-f8e78c9af11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_heads = 2\n",
    "\n",
    "att_head = AttentionHead(\n",
    "    embed_dim=test_embeddings.shape[-1],\n",
    "    head_dim=test_embeddings.shape[-1] / n_heads\n",
    ")\n",
    "\n",
    "att_head(test_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a7d360-8089-46b1-bc1e-00e015a43dd9",
   "metadata": {},
   "source": [
    "Define a multi-head attention layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2820e8d9-592f-44ee-957b-cf891a057ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "from encoder import MultiHeadAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bd57dd-073b-4fcf-b5a9-70c0d0f14789",
   "metadata": {},
   "source": [
    "Test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb87bf2d-7d47-4e94-a15d-c17b02cb6755",
   "metadata": {},
   "outputs": [],
   "source": [
    "mah_layer = MultiHeadAttention(config=config)\n",
    "\n",
    "mah_layer(test_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0561fa-8b6c-4cf6-a1e9-48e147888bd4",
   "metadata": {},
   "source": [
    "## Final feed-forward (FFN) layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1ccf6d-673a-4482-936f-2392f371447c",
   "metadata": {},
   "source": [
    "The FFN layer is a fully-connected feed-forward layer put after the MHA layer, with the architecture of a __position-wise feed-forward layer__, i.e. processing each token embedding outputted by the MHA layer __independently from the others__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08f4fab-165b-4ba9-be9f-aec860cd1fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from encoder import FeedForward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3012c0e-a4f4-4758-9a8e-9ebc7e247f5d",
   "metadata": {},
   "source": [
    "Test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b442f0f-bc7a-4306-a623-0115420a9c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_forward = FeedForward(config=config)\n",
    "\n",
    "feed_forward(\n",
    "    mah_layer(test_embeddings)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e17ab7-4a6b-4fe4-8fc9-45e29120236c",
   "metadata": {},
   "source": [
    "## Layer normalization and skip connection: building the full encoder layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6ee5bc-709f-4693-ab97-d053bfc21952",
   "metadata": {},
   "source": [
    "The full encoder layer will have both an MHA and an FFN layer, but on top of these will also include __layer normalization__ and __skip connections__.\n",
    "\n",
    "Layer normalization can happen __pre-layer__ or __post_layer__, according to where the layer normalization operation is put w.r.t. the skip connections. We'll implement __pre-layer normalization__, which is more numerically stable during training.\n",
    "\n",
    "__Note:__ the input and output shapes of the encoder are __the same__ - the operations performed are not about altering the shape, but rather adding contextual information without changing the sape itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df51d7fd-4e5e-48dc-af3b-c9216709bd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from encoder import TransformerEncoderLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b76568a-f35a-415b-aec1-da0c53f80c07",
   "metadata": {},
   "source": [
    "Test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5835c3d-dffa-4d81-bc09-ccc32925887b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layer = TransformerEncoderLayer(config=config)\n",
    "\n",
    "encoder_layer(test_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e09ab4-10fd-4f34-9232-d7bf3c64322c",
   "metadata": {},
   "source": [
    "## Building the full transformer encoder as a stack of encoder layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10d722e-85aa-46bd-b201-3e34ffd287fc",
   "metadata": {},
   "source": [
    "The encoder part of a transformer is composed of a stack (sequence) of encoder layers (as defined before) the data goes through before being outputted. Let's build it, including also the embedding part (which makes sense as it's trainable and therefore must be trained along with the rest of the model: they are one whole thing together)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1645ada2-70fe-4691-baa0-ecac2c6eac4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from encoder import TransformerEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3957069-8651-4f6d-bffc-38fde457ec76",
   "metadata": {},
   "source": [
    "Test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a547504-241c-42df-b84d-2ea47633ea7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = TransformerEncoder(config=config)\n",
    "\n",
    "encoder(test_token_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
