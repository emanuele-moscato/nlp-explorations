{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "064e30ac-23c8-412e-a9d3-3cc6fbfc56b7",
   "metadata": {},
   "source": [
    "# Implementation of a decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab428806-b281-4b0a-bf91-a67f5cb85da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from transformers import AutoConfig, AutoTokenizer\n",
    "\n",
    "sys.path.append('../modules/')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f121b415-8d4a-4804-9c27-3b00e5959fb4",
   "metadata": {},
   "source": [
    "## Load config and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db18dd8-5a9c-453a-8041-29a61119f2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a model (checkpoint).\n",
    "model_ckpt = 'distilbert-base-uncased'\n",
    "\n",
    "# Load the model's config.\n",
    "config = AutoConfig.from_pretrained(model_ckpt)\n",
    "\n",
    "# Load the tokenizer associated to the model.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345095dd-e5a2-4ac4-bb84-a3bfaf0699cd",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc11e23-293a-4aa9-844e-b6e8f9348b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\n",
    "    \"To search for perfection\",\n",
    "    \"Is all very well\",\n",
    "    \"But to look for heaven\",\n",
    "    \"Is to live here in hell\"\n",
    "]\n",
    "\n",
    "input_ids = tokenizer(\n",
    "    text,\n",
    "    padding=True,\n",
    "    return_tensors='tf'\n",
    ")['input_ids']\n",
    "\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d53d6f8-3ad4-46eb-84ef-31f1609171ea",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c2e8bb-082b-44b2-9223-44bbd91ffda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from encoder import Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a806164-d25d-4459-9b9b-7b6b61b6080c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = Embeddings(config=config)\n",
    "\n",
    "token_embeddings = embeddings(input_ids)\n",
    "\n",
    "token_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcda865-c4a1-4556-9b64-c39a57c79a83",
   "metadata": {},
   "source": [
    "## Masked multi-head self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ecf886-a9ca-4304-909d-4b8aaf2e043a",
   "metadata": {},
   "source": [
    "The masked multi-head self-attention layer prevents the decoder from seeing tokens it still has to predict during the training phase (in a sequence-to-sequence task, e.g. machine translation, the decoder predicts the output tokens sequentially and during training these are compared with the true sequence to predict, the errors being then backpropagated to update the weights of the model)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffd84e8-9085-48fc-b64e-6ad7dfec42e6",
   "metadata": {},
   "source": [
    "Build a mask for the attention scores matrix: the way tokens are masked in the input sequence to the decoder is by taking the attention scores matrix (__before softmax is applied__) and set entries in the upper triangle (excluding the diagonal, which corresponds to the \"current token\") to $-\\infty$. This way those entries will be mapped to 0 by softmax and the corresponding attention weights will also be 0. The upper triangle (excluding the diagonal) corresponds indeed to all the tokens __after__ the given ones (corresponding to the row index) in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4888f462-8bf2-4cb6-a8d7-1420dc590c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = input_ids.shape[-1]\n",
    "\n",
    "mask = 1. - tf.linalg.band_part(\n",
    "    tf.ones(shape=(seq_len, seq_len)),\n",
    "    num_lower=-1,\n",
    "    num_upper=0\n",
    ")\n",
    "\n",
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5481c6-2ce8-41e5-af91-0e75d9608dc2",
   "metadata": {},
   "source": [
    "Randomly generate a fake atetntion score matrix, for testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892508c7-d18d-4caf-9b78-e7dffd9f762d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fake score matrix.\n",
    "fake_scores = tf.random.normal(shape=(seq_len, seq_len))\n",
    "\n",
    "fake_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366ec989-1dea-4dc5-8baa-97d27b03c09f",
   "metadata": {},
   "source": [
    "Apply the masking to the fake attention scores matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c80b7a-cdf9-4cc9-8424-baf5fef3f36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_fake_scores = np.where(mask == 1, - mask * np.infty, fake_scores)\n",
    "\n",
    "masked_fake_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf6715b-ed20-4dbb-977b-591fbd7264b0",
   "metadata": {},
   "source": [
    "## Scaled dot-product attention with masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62026932-64a2-4b99-9edb-a9e74e2f2817",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import masked_scaled_dot_product_attention\n",
    "from encoder import Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a6e9ef-e52f-450a-8beb-dbaf13d31978",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = Embeddings(config=config)\n",
    "\n",
    "token_embeddings = embeddings(input_ids)\n",
    "\n",
    "token_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1e916a-f428-468f-80f1-cfffdfc2cd9c",
   "metadata": {},
   "source": [
    "Generate fake query, key and value vectors and test scaled dot-product attention. Indeed, if we return the weights instead of the linear combinations of the value vectors, we see all the zeros generated by softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c879e389-9318-43db-b756-795462398a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8aa8a85-2a18-428d-9178-ea3323ca88f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_q = Dense(units=config.dim)(token_embeddings)\n",
    "fake_k = Dense(units=config.dim)(token_embeddings)\n",
    "fake_v = Dense(units=config.dim)(token_embeddings)\n",
    "\n",
    "masked_scaled_dot_product_attention(fake_q, fake_k, fake_v, return_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759a154c-88ce-455a-b397-e2520302755e",
   "metadata": {},
   "source": [
    "## Masked single-head self-attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3c4001-9b9c-47f4-9158-30c588c5fd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from decoder import MaskedAttentionHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efbd198-2f1a-457d-895c-b5b52b57ab7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_attention_head = MaskedAttentionHead(\n",
    "    embed_dim=config.hidden_size,\n",
    "    head_dim=config.hidden_size // 1 # One head.\n",
    ")\n",
    "\n",
    "masked_attention_head(token_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d6a11d-5b16-49f4-addb-bb8e6235839c",
   "metadata": {},
   "source": [
    "## Masked multi-head self-attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f1c89c-11b5-4877-8d02-b8ba19bec899",
   "metadata": {},
   "outputs": [],
   "source": [
    "from decoder import MaskedMultiHeadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8193506e-2181-45a7-b003-3c5b2a5f07e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_multi_head_attention = MaskedMultiHeadAttention(config=config)\n",
    "\n",
    "decoder_hidden_states = masked_multi_head_attention(token_embeddings)\n",
    "\n",
    "decoder_hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ecc979-7cec-4021-a097-37585be73bd6",
   "metadata": {},
   "source": [
    "## Encoder-decoder single-head attention layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11afa5cc-c9fe-4e67-b9db-6b30df7a5fe8",
   "metadata": {},
   "source": [
    "The encoder-decoder single-head attention layer is an attention layer that computes attention weights between hidden states coming from the decoder and the key vectors coming from the decoder. These weights are then used to take linear combinations of the value vectors __coming from the encoder__.\n",
    "\n",
    "Because sequences coming from the decoder and the encoder inputs can have different lengths, in general the weights matrix will be rectangular rather than square (as in self-attention). On the other hand, because similarity is still based on computing dot products, after projecting down to the head dimension the query, key and value vactors must have the same size, but this need not be true before the projection!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45a07dc-8c7b-484c-b4a2-a5d3001334fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546decb9-86b2-40e2-8dea-7d0dbfd0ae7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_heads = 12\n",
    "head_dim = config.hidden_size // n_heads\n",
    "\n",
    "head_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad18a8b6-f8b0-451e-bd50-06a8d85efd04",
   "metadata": {},
   "source": [
    "Generate fake key and value vectors coming from the encoder. For simplicity, we are assuming the encoder and decoder use embeddings with the same dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61575e89-0584-4418-b235-01e1fbe4b591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to have batches of the same dimension in the\n",
    "# encoder and decoder inputs, otherwise the scaled dot-product\n",
    "# attention computation doesn't work (error with the\n",
    "# batch dimension not matching).\n",
    "n_sequences_encoder = input_ids.shape[0]\n",
    "seq_len_encoder = 5  # Different sequence length w.r.t. to the decoder.\n",
    "\n",
    "fake_encoder_k = tf.random.normal(shape=(n_sequences_encoder, seq_len_encoder, config.hidden_size))\n",
    "fake_encoder_v = tf.random.normal(shape=(n_sequences_encoder, seq_len_encoder, config.hidden_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be40838-627b-4708-9fd4-6f3283a9bd9d",
   "metadata": {},
   "source": [
    "Project query, key and value vectors to the head dimension using dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22eb8ed-0fbd-4414-b3f4-6591924a084c",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_q = Dense(units=head_dim)(decoder_hidden_states)\n",
    "head_k = Dense(units=head_dim)(fake_encoder_k)\n",
    "head_v = Dense(units=head_dim)(fake_encoder_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fc67c2-cba5-4ea7-a902-555fe62bc7bb",
   "metadata": {},
   "source": [
    "Compute scaled dot-product attention between the key vectors coming from the encoder and the hidden states coming from the decoders (used as the query vectors). Indeed the weights matrix is rectangular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6414d52-afc0-463a-92d7-3bcfa7066209",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import scaled_dot_product_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b997d70-488b-4250-9c2a-3f4880d4a707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output shape: (batch_size, seq_len_decoder, seq_len_encoder).\n",
    "scaled_dot_product_attention(\n",
    "    query=head_q,\n",
    "    key=head_k,\n",
    "    value=head_v,\n",
    "    return_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a581833-5aa7-43d1-baa9-fef2c6b18354",
   "metadata": {},
   "source": [
    "Linear combination of the encoder value vectors with weights equal to the attention weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0506e18-6c85-4dd9-887a-75acd398b76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output shape: (batch_size, seq_len_decoder, head_dim).\n",
    "scaled_dot_product_attention(\n",
    "    query=head_q,\n",
    "    key=head_k,\n",
    "    value=head_v\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249fbabd-f410-44aa-a552-369f2748e3f6",
   "metadata": {},
   "source": [
    "Test the class defined in the module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e93123-8919-4eb2-85d6-f58b6838e168",
   "metadata": {},
   "outputs": [],
   "source": [
    "from decoder import EncoderDecoderAttentionHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056b94a7-de62-4123-b5b0-0a2d943b6083",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_decoder_att_head = EncoderDecoderAttentionHead(head_dim=head_dim)\n",
    "\n",
    "encoder_decoder_att_head(\n",
    "    decoder_hidden_state=decoder_hidden_states,\n",
    "    encoder_k=fake_encoder_k,\n",
    "    encoder_v=fake_encoder_v\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762ccd2a-a5dc-4104-b9e2-852a34329016",
   "metadata": {},
   "source": [
    "## Encoder-decoder multi-head attention layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e35133-8a8a-439d-89cb-a6a331fed4b3",
   "metadata": {},
   "source": [
    "Simply test the class defined in the module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bff2d5-702f-466f-b7f6-d9b1847520e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from decoder import EncoderDecoderMultiHeadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bae306-6f4e-4154-a2c0-376a877fc70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_decoder_multi_head_att = EncoderDecoderMultiHeadAttention(config=config)\n",
    "\n",
    "encoder_decoder_multi_head_att(\n",
    "    decoder_hidden_state=decoder_hidden_states,\n",
    "    encoder_k=fake_encoder_k,\n",
    "    encoder_v=fake_encoder_v\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e958e115-f1fd-463f-ada5-803a4563b684",
   "metadata": {},
   "source": [
    "## Test a single decoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bfd58c-e39e-4330-a6ef-d290bfd42145",
   "metadata": {},
   "outputs": [],
   "source": [
    "from decoder import TransformerDecoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7937128-7652-45e0-bd89-8c240b743588",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_layer = TransformerDecoderLayer(config=config)\n",
    "\n",
    "decoder_layer(token_embeddings, fake_encoder_k, fake_encoder_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874788b4-9545-4fd3-b280-ea2daeaf27b9",
   "metadata": {},
   "source": [
    "## Test a full decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c209449-843f-422b-b12a-2fe794a436ce",
   "metadata": {},
   "source": [
    "A full decoder is given by an embedding layer (same architecture as the one used for the encoder) followed by a stack (sequence) of decoder layers, as seen above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a00543e-b50a-40ed-a576-000e58520f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from decoder import TransformerDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc7c9ce-f16c-438e-b77f-2323463502f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = TransformerDecoder(config=config)\n",
    "\n",
    "decoder(input_ids, fake_encoder_k, fake_encoder_v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
