{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58346a2d-52db-4b86-8cbc-261daa4c44d2",
   "metadata": {},
   "source": [
    "# Test an ecoder-only model built for a classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b003a395-d52e-4d8d-88c2-f78ddf390a73",
   "metadata": {},
   "source": [
    "__Objective:__ test the implementation of an encoder-only model with a classification head (a dense layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6041bf1c-07e8-45d3-8c1e-31159bf7d371",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from transformers import AutoConfig, AutoTokenizer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model\n",
    "\n",
    "sys.path.append('../modules/')\n",
    "\n",
    "from encoder_text_classifier import TransformerForSequenceClassification\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aff76f7-6702-4d36-ad84-5745d8039ebd",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51985ef-ebce-4dc6-9691-41f5c45325ce",
   "metadata": {},
   "source": [
    "Load config for the model (in this case we refer to a pretrained model just to get the values for the hyperparameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8099fcbc-9400-442a-994f-510155ecd639",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = 'distilbert-base-uncased'\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187f5b4a-a3a6-4576-8687-7d319c39eb06",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a308714-9320-4be1-bc92-1186ad243e97",
   "metadata": {},
   "source": [
    "Instantiate a tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b092e6e9-ebe3-4184-a52a-22cd1742b6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b8eb3b-590f-4317-b037-30030c3ba888",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\n",
    "    \"Can't hear what he's saying\",\n",
    "    \"when he's talking in his sleep\",\n",
    "    \"He finally found the sound\",\n",
    "    \"but he's in too deep\"\n",
    "]\n",
    "\n",
    "token_ids = tokenizer(\n",
    "    text,\n",
    "    padding=True,\n",
    "    return_tensors='tf'\n",
    ")['input_ids']\n",
    "\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12773da-7ae8-44ec-b9f8-920ecfe5238d",
   "metadata": {},
   "source": [
    "Check converting IDs back to tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4477754-2c4a-4e70-8cf7-9ff142a01afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(token_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8573ee21-d260-4620-8ed5-dba16fcc53ad",
   "metadata": {},
   "source": [
    "## Test the forward pass of the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2354c7ad-b5e8-4477-9046-9cbe19189d7c",
   "metadata": {},
   "source": [
    "Instantiate the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14afcd60-039a-4dfb-a666-d5cf593d9728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add missing parameters (ormodify some) in the config.\n",
    "config.hidden_dropout_prob = 0.1\n",
    "config.num_labels = 3\n",
    "\n",
    "encoder_classifier = TransformerForSequenceClassification(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ded44ac-95e5-40e8-933c-ccc7271105aa",
   "metadata": {},
   "source": [
    "Test the forward pass of the classifier.\n",
    "\n",
    "__Note:__ because of how it's implemented, for each sample in the batch the model returns the unnormalize logits for each possible class (we should apply softmax to get probabilities over the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6053750-93dc-46f7-b354-c6ecc28f0eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_classifier(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed1c1da-4b0e-40cc-a348-833798018fd3",
   "metadata": {},
   "source": [
    "## Build a model from the classifier layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22e7777-d426-46d6-8a25-f18ad6cec424",
   "metadata": {},
   "source": [
    "The `TransformerForSequenceClassification` is a subclass of Keras' `Layer`, so it's a layer object, not a model. If we want to fit the model we have to build a Keras model from it, which can be done with the functional API specifying inputs (paceholder with the right shape) and outputs.\n",
    "\n",
    "__Notes:__\n",
    "- Keras' `Input` object needs to be passed a shape that __does not include the batch shape__.\n",
    "- The loss function is chosen just for code testing purposes, for it to make sense we should map the logits to the probabilities (with `softmax`) and then use categorical cross-entropy. Also, we are generating fake targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922e4d3c-35cb-4144-9854-ae76eadddfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(\n",
    "    shape=(token_ids.shape[-1],)\n",
    ")\n",
    "\n",
    "outputs = encoder_classifier(inputs)\n",
    "\n",
    "model = Model(\n",
    "    inputs=inputs,\n",
    "    outputs=outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80895025-b3e5-4e10-bfba-912c712bda8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss='mse',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7921bc32-9c96-437c-b6ec-0a7cb5619574",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of parameters in the model:', model.count_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ffd68c-d2f8-437e-a7f2-7a9a2b649b23",
   "metadata": {},
   "source": [
    "Generate fake targets and fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3299082-cb45-4a0c-a11d-1c2572aa27d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_targets = tf.ones_like(model(token_ids))\n",
    "\n",
    "model.fit(\n",
    "    x=token_ids,\n",
    "    y=fake_targets,\n",
    "    epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bb320b-63d5-4b5f-bd68-9886bab1c8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(token_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
