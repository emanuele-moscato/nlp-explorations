{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c64cf3d-7244-4b01-8200-8f4d9b3ae54e",
   "metadata": {},
   "source": [
    "# Implementing a full transformer encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a38f829-8406-499f-b17a-4f68bc73eaf9",
   "metadata": {},
   "source": [
    "__Objective:__ implement the encoder part of a transformer model from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be906773-3924-4b3c-85da-09a14657e85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Layer, Dense, Dropout, LayerNormalization\n",
    "from tensorflow.keras.activations import gelu\n",
    "from transformers import AutoTokenizer, AutoConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a552ea6-646a-4bca-ad17-6d14ddcd29bd",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9838d82-6772-44da-be3c-4e140eda5fc6",
   "metadata": {},
   "source": [
    "An attention layer works with __token embeddings__ as the input, so we need to start by tokenizing the input text and creating the vector embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801f3776-81cc-4861-bd87-225fa2f8f76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a tokenizer from a model.\n",
    "model_ckpt = 'distilbert-base-uncased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f245ef09-319f-4fd0-8616-09e36ab77003",
   "metadata": {},
   "source": [
    "Test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21573b4e-7634-44fd-abc7-728a691e2199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the tokenizer.\n",
    "test_text = \"\"\"\n",
    "I know all about the honor of God, Mary Jane.\n",
    "\"\"\"\n",
    "\n",
    "test_output = tokenizer(\n",
    "    test_text,\n",
    "    return_tensors='tf',\n",
    "    padding=True,\n",
    "    # In this case we exclude the start- and end-of-sentence tokens.\n",
    "    add_special_tokens=False\n",
    ")\n",
    "\n",
    "test_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6b4b78-4cca-46b4-b2d5-6b4af99fa7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output['input_ids'].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c118a5-bb29-4138-a5e2-555909f804bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(test_output['input_ids'][0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f81aa5-e425-4246-96d3-d590b791e3e8",
   "metadata": {},
   "source": [
    "## Creation of embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254112bc-b95e-4708-8305-071d8cde6fe5",
   "metadata": {},
   "source": [
    "Create the word embeddings (vectors) from the tokenized text.\n",
    "\n",
    "Keras' `Embedding` layer maps positive integers (tokenized text) to dense vectors of fixed size.\n",
    "\n",
    "__Notes:__\n",
    "- At this point the embeddings of the tokens know nothing about the context - each token's embedding is always the same, __irrespective of the context__ (i.e. the embedding operation is deterministic). The attention layer is there right to modify the embeddings to include context-depending information.\n",
    "- We skip positional encoding for simplicity, but the information thereof should be added to the token embeddings at this point!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d30bfc5-03a0-4554-9423-8156758bef50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the configuration parameters of the pretrained model.\n",
    "config = AutoConfig.from_pretrained(model_ckpt)\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e996b98-e996-44eb-8aa6-1919fa48c475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the embedding layer.\n",
    "token_emb = Embedding(\n",
    "    input_dim=config.vocab_size,  # We could have used tokenizer.vocab_size, it's the same.\n",
    "    output_dim=config.hidden_size\n",
    ")\n",
    "\n",
    "token_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b423ce-ca2a-4bc0-8138-8b69b6070ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the creation of embedding for some tokenized text.\n",
    "# Output shape: (batch_size, seq_len, hidden_dim).\n",
    "test_embeddings = token_emb(test_output['input_ids'])\n",
    "\n",
    "test_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0927a2-59a7-4b4d-8af2-d65f1bb06173",
   "metadata": {},
   "source": [
    "### Add positional encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b92e3f-00b4-46f3-9cc7-35de331dc484",
   "metadata": {},
   "source": [
    "We now add positional encoding to the embeddings, so each embedding also contains information of the position of the corresponding token in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f128a2-7a99-4f59-85c7-4eb0dcdaa437",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(Layer):\n",
    "    \"\"\"\n",
    "    Class implementing the embedding layer, with output\n",
    "    embeddings incorporating both information from token\n",
    "    and from position embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        Initializes the inner layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize all the inner layers.\n",
    "        # Simple dense embedding of the numerical tokens.\n",
    "        self.token_embeddings = Embedding(\n",
    "            input_dim=config.vocab_size,\n",
    "            output_dim=config.hidden_size\n",
    "        )\n",
    "\n",
    "        # For the positional embedding, we use the Keras\n",
    "        # `Embedding` layer again, this time with an input\n",
    "        # dimension equal to the maximum positional embedding\n",
    "        # (the maximum index of a token within a sequence, closely\n",
    "        # related to the maximum length of a sequence) rather\n",
    "        # than to the size of the vocabulary.\n",
    "        self.position_embeddings = Embedding(\n",
    "            input_dim=config.max_position_embeddings,\n",
    "            output_dim=config.hidden_size\n",
    "        )\n",
    "\n",
    "        self.layer_norm = LayerNormalization(epsilon=1e-12)\n",
    "\n",
    "        self.dropout = Dropout(rate=config.dropout)\n",
    "\n",
    "    def call(self, input_ids):\n",
    "        \"\"\"\n",
    "        Forward pass of the embedding layer.\n",
    "        Input: token IDs.\n",
    "        Output: embeddings.\n",
    "\n",
    "        Token and posiiton embeddings are generated for the\n",
    "        input IDs and then added up. The resulting embeddings\n",
    "        are then normalized with layer normalization and regularized\n",
    "        with a dropout layer.\n",
    "        \"\"\"\n",
    "        # Get the sequence length.\n",
    "        seq_length = input_ids.shape[1]\n",
    "\n",
    "        # Get all the position IDs as the rage from 0 to seq_length - 1.\n",
    "        position_ids = tf.range(\n",
    "            seq_length,\n",
    "            dtype=tf.int64\n",
    "        )\n",
    "\n",
    "        # Create token embeddings.\n",
    "        token_embeddings = self.token_embeddings(input_ids)\n",
    "\n",
    "        # Create position embeddings.\n",
    "        position_embeddings = self.position_embeddings(position_ids)[tf.newaxis, ...]\n",
    "\n",
    "        # Combine the information in token and position embeddings\n",
    "        # by adding them up. The shape of `position_embeddings` is\n",
    "        # broadcast to that of `token_embeddings`.\n",
    "        embeddings = token_embeddings + position_embeddings\n",
    "\n",
    "        # Normalize the combined embeddings.\n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "\n",
    "        # Dropout regularization on the embeddings.\n",
    "        embeddings = self.dropout(embeddings)\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04312b7-45ec-4790-8689-beeeb615332e",
   "metadata": {},
   "source": [
    "Test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f7d40d-9eac-4254-b113-2e62ec3c0c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\n",
    "    \"Six o' clock on the Christmas morning...\",\n",
    "    \"...and for what?\"\n",
    "]\n",
    "\n",
    "embedding_layer = Embeddings(config=config)\n",
    "\n",
    "test_embeddings = embedding_layer(\n",
    "    tokenizer(\n",
    "        text,\n",
    "        return_tensors='tf',\n",
    "        padding=True,\n",
    "        add_special_tokens=True\n",
    "    )['input_ids']\n",
    ")\n",
    "\n",
    "test_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902b79a9-e55f-4079-a94c-bfbded33117c",
   "metadata": {},
   "source": [
    "## A basic self-attention mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f6e212-bc48-4d9f-9526-647ea1fe4f8c",
   "metadata": {},
   "source": [
    "We reproduce the basic operations for a single-head attention layer, acting on the test embeddings obtained above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8194876-c05c-4753-964a-09be56832a2b",
   "metadata": {},
   "source": [
    "### Creation of query, key and value vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f0961c-c17e-4487-a0c7-51abcdc739a4",
   "metadata": {},
   "source": [
    "For simplicity, we can take the query, key and value vectors associated to each token embedding equal to the token embedding itself (and thus also equal to one another). This need not be the case: in general, independent weight matrices (__trainable__) are applied to get the query, key and value vectors from the token embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ed8c39-4c04-4d8d-88ed-d1a74d01538f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = test_embeddings\n",
    "key = test_embeddings\n",
    "value = test_embeddings\n",
    "\n",
    "dim_k = key.shape[-1]\n",
    "\n",
    "dim_k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173cbbb4-0391-4f04-86c5-87e245d6fb51",
   "metadata": {},
   "source": [
    "### Attention scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696a0086-0577-4b6a-842c-6df6c714ab01",
   "metadata": {},
   "source": [
    "Given an input, the attention scores (not the weights yet!) are computed as the dot product of each query vector with each key vector. This measures the similarity (relevance) of each key w.r.t. each query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10105526-9b6e-4604-8d3e-135fb74fa001",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.shape, key.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85db8d14-8ed6-496e-82b7-208bdacdf1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output shape: (batch_size, seq_len, seq_len).\n",
    "scores = tf.matmul(\n",
    "    query,\n",
    "    # Leaving the batch shape as the first dimension, it's ignored\n",
    "    # in the matrix multiplication.\n",
    "    tf.transpose(key, perm=(0, 2, 1))\n",
    ")\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb52d37d-71d2-4253-8e22-8e137f85598a",
   "metadata": {},
   "source": [
    "### Attention weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90649227-9b90-428f-b90b-164034754198",
   "metadata": {},
   "source": [
    "Attention weights are obtained from attention scores by:\n",
    "1. Rescaling the scores dividing by $\\sqrt{\\text{hidden dim}}$. This is done to avoid too large scores, which would mess up with the gradient descent steps in the training phase.\n",
    "2. Applying the `softmax` function to the last axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d85f777-7831-44ab-b40a-37491d9e9db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = tf.math.softmax(\n",
    "    scores / tf.sqrt(tf.cast(dim_k, tf.float32)),\n",
    "    axis=-1\n",
    ")\n",
    "\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ee187c-7829-41d9-bc83-45d0c29e1186",
   "metadata": {},
   "source": [
    "Check: row by row, if we add up all the entries in the columns we should get a value close to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a136fe-19ff-4a24-84aa-b2a4ec8eebb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_sum(weights, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007206c7-b2be-4f1a-a62e-af18a30c4030",
   "metadata": {},
   "source": [
    "### Output of the self-attention layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b838e8-42b1-4a6a-bce9-c620a945bb66",
   "metadata": {},
   "source": [
    "The output of the layer is a linear combination of the value vectors with weights given by the attention weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8c4000-cbf3-4c12-b839-bab075a44de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output shape: (batch_size, seq_len, value_size).\n",
    "test_attention_output = tf.matmul(weights, value)\n",
    "\n",
    "test_attention_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04294934-21cc-4317-b5de-72968b82440d",
   "metadata": {},
   "source": [
    "## Multi-headed attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a0e578-2f9a-4b70-be3f-0ef0ee2e6fa0",
   "metadata": {},
   "source": [
    "Implement a multi-head attention layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f28b3b-1916-4522-bf1d-4e45760d335c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value):\n",
    "    \"\"\"\n",
    "    Implements the scaled dot product attention operation.\n",
    "    \"\"\"\n",
    "    # Dimension of the key vectors.\n",
    "    dim_k = key.shape[-1]\n",
    "\n",
    "    # Compute the attention scores.\n",
    "    scores = tf.matmul(\n",
    "        query,\n",
    "        # Leaving the batch shape as the first dimension, it's ignored\n",
    "        # in the matrix multiplication.\n",
    "        tf.transpose(key, perm=(0, 2, 1))\n",
    "    )\n",
    "\n",
    "    # Compute the attention weights.\n",
    "    weights = tf.math.softmax(\n",
    "        scores / tf.sqrt(tf.cast(dim_k, tf.float32)),\n",
    "        axis=-1\n",
    "    )\n",
    "\n",
    "    # Return a linear combination of the value vectors\n",
    "    # with weights equal to the attention weights.\n",
    "    return tf.matmul(weights, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6e8eb3-eb0e-4993-a378-f1a1dc668348",
   "metadata": {},
   "source": [
    "Define a single attention head layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5be73b6-4dd9-49d1-8859-9458d1cc234a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(Layer):\n",
    "    \"\"\"\n",
    "    Implementation of a single-head self-attention layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, head_dim):\n",
    "        \"\"\"\n",
    "        Initializes the layers that project the input to the\n",
    "        AttentionHead layer onto the corresponding query (q),\n",
    "        key (k) and value (v) vectors.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        embed_dim : int\n",
    "            Size of the input embeddings. This is established by\n",
    "            the particular token embedding chosen.\n",
    "        head_dim : int\n",
    "            Size of the output of the AttentionHead layer. In\n",
    "            multi-head attention this will be < embed_dim, and\n",
    "            the full dimension of the embeddings is recovered\n",
    "            when the outputs of each head are concatenated back\n",
    "            together.\n",
    "        \"\"\"\n",
    "        # Execute the parent class' constructor.\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize the query, key and value weighting matrices.\n",
    "        self.q = Dense(units=head_dim)\n",
    "        self.k = Dense(units=head_dim)\n",
    "        self.v = Dense(units=head_dim)\n",
    "\n",
    "    def call(self, hidden_state):\n",
    "        \"\"\"\n",
    "        Forward pass of the layer. The query, key and value\n",
    "        vectors are computed applying the q, k and v layers\n",
    "        to the input.\n",
    "\n",
    "        Input shape: (batch_shape, seq_len, embed_dim)\n",
    "        Ouput shape: (batch_shape, seq_len, head_dim)\n",
    "        \"\"\"\n",
    "        attn_outputs = scaled_dot_product_attention(\n",
    "            self.q(hidden_state),\n",
    "            self.k(hidden_state),\n",
    "            self.v(hidden_state)\n",
    "        )\n",
    "\n",
    "        return attn_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bde0fc-ccc0-4e7d-8063-453e64d52b31",
   "metadata": {},
   "source": [
    "Test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7109f1b-7cd4-43b1-86f4-f8e78c9af11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_heads = 2\n",
    "\n",
    "att_head = AttentionHead(\n",
    "    embed_dim=test_embeddings.shape[-1],\n",
    "    head_dim=test_embeddings.shape[-1] / n_heads\n",
    ")\n",
    "\n",
    "att_head(test_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a7d360-8089-46b1-bc1e-00e015a43dd9",
   "metadata": {},
   "source": [
    "Define a multi-head attention layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0a621c-75cf-475b-b955-01f38d604823",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(Layer):\n",
    "    \"\"\"\n",
    "    Implementation of a multi-head self-attention layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        Initializes a list of single-head self-attention layers and\n",
    "        the final dense (fully-connected) layer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        embed_dim = config.hidden_size\n",
    "        num_heads = config.num_attention_heads\n",
    "        head_dim = embed_dim // num_heads\n",
    "\n",
    "        # Initialize a list of attention heads.\n",
    "        self.heads = [\n",
    "            AttentionHead(embed_dim=embed_dim, head_dim=head_dim)\n",
    "            for _ in range(num_heads)\n",
    "        ]\n",
    "\n",
    "        # Initialize the final dense layer.\n",
    "        self.output_linear = Dense(units=embed_dim)\n",
    "\n",
    "    def call(self, hidden_state):\n",
    "        \"\"\"\n",
    "        Forward pass: the input is passed through each head\n",
    "        independently, then the outputs are concatenated back\n",
    "        together and passed through the final dense layer.\n",
    "\n",
    "        Input shape: (batch_shape, seq_len, hidden_dim)\n",
    "        Output layer: (batch_shape, seq_len, hidden_dim)\n",
    "        \"\"\"\n",
    "        # Pass the input through each head and concatenate\n",
    "        # the outputs.\n",
    "        x = tf.concat(\n",
    "            [h(hidden_state) for h in self.heads],\n",
    "            axis=-1\n",
    "        )\n",
    "\n",
    "        # Pass the concatenated outputs through the final\n",
    "        # linear layer.\n",
    "        x = self.output_linear(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bd57dd-073b-4fcf-b5a9-70c0d0f14789",
   "metadata": {},
   "source": [
    "Test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb87bf2d-7d47-4e94-a15d-c17b02cb6755",
   "metadata": {},
   "outputs": [],
   "source": [
    "mah_layer = MultiHeadAttention(config=config)\n",
    "\n",
    "mah_layer(test_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0561fa-8b6c-4cf6-a1e9-48e147888bd4",
   "metadata": {},
   "source": [
    "## Final feed-forward (FFN) layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1ccf6d-673a-4482-936f-2392f371447c",
   "metadata": {},
   "source": [
    "The FFN layer is a fully-connected feed-forward layer put after the MHA layer, with the architecture of a __position-wise feed-forward layer__, i.e. processing each token embedding outputted by the MHA layer __independently from the others__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9d48ae-c499-4ac0-bcab-63e2d32ea622",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(Layer):\n",
    "    \"\"\"\n",
    "    Implementation of the feed-forward layer to be added\n",
    "    after the MHA layers (both in the encoder and in the\n",
    "    decoder part of a transformer).\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        Initializes the inner layers and activation function.\n",
    "        Rule of thumb for the intermediate size: 4 * [hidden_size].\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_1 = Dense(\n",
    "            units=config.hidden_dim,\n",
    "            activation='gelu'\n",
    "        )\n",
    "        self.linear_2 = Dense(units=config.hidden_size)\n",
    "\n",
    "        self.dropout = Dropout(rate=config.dropout)\n",
    "\n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the layer. Sequentially, the input passes\n",
    "        through:\n",
    "          1. A dense layer with GELU activation function.\n",
    "          2. Another dense layer with identity activation.\n",
    "          3. A dropout regularization layer.\n",
    "\n",
    "        Input shape: (batch_size, seq_len, hidden_dim)\n",
    "        Output shape: (batch_size, seq_len, hidden_dim)\n",
    "\n",
    "        Note: by default, the Dense layers act on the last (right-most)\n",
    "              dimension of an input tensor, leaving any other dimension\n",
    "              untouched - which is exactly what we want to process\n",
    "              each embedding independently from the others.\n",
    "        \"\"\"\n",
    "        x = self.linear_1(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3012c0e-a4f4-4758-9a8e-9ebc7e247f5d",
   "metadata": {},
   "source": [
    "Test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b442f0f-bc7a-4306-a623-0115420a9c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_forward = FeedForward(config=config)\n",
    "\n",
    "feed_forward(\n",
    "    mah_layer(test_embeddings)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e17ab7-4a6b-4fe4-8fc9-45e29120236c",
   "metadata": {},
   "source": [
    "## Layer normalization and skip connection: building the full encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6ee5bc-709f-4693-ab97-d053bfc21952",
   "metadata": {},
   "source": [
    "The full encoder will have both an MHA and an FFN layer, but on top of these will also include __layer normalization__ and __skip connections__.\n",
    "\n",
    "Layer normalization can happen __pre-layer__ or __post_layer__, according to where the layer normalization operation is put w.r.t. the skip connections. We'll implement __pre-layer normalization__, which is more numerically stable during training.\n",
    "\n",
    "__Note:__ the input and output shapes of the encoder are __the same__ - the operations performed are not about altering the shape, but rather adding contextual information without changing the sape itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e9a26e-1d3f-4151-a9b4-d2bb4b32503a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(Layer):\n",
    "    \"\"\"\n",
    "    Class implementing the full encoder part of a transformer.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        Initializes all the inner layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize all the layers.\n",
    "        self.layer_norm_1 = LayerNormalization()\n",
    "        self.layer_norm_2 = LayerNormalization()\n",
    "        self.attention = MultiHeadAttention(config)\n",
    "        self.feed_forward = FeedForward(config)\n",
    "\n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass: the input passes through the MHA and FFN\n",
    "        layers, with layer normalization and skip connections\n",
    "        in between. Pre-layer normalization is used.\n",
    "        \"\"\"\n",
    "        # Compute the normalized input.\n",
    "        # Note: we don't put this back into the x variable\n",
    "        #       because we need to have them both for skip\n",
    "        #       connections.\n",
    "        hidden_state = self.layer_norm_1(x)\n",
    "\n",
    "        # Skip connection: the input to the first layer normalization\n",
    "        # is added to the output obtained from the MHA layer acting\n",
    "        # on the normalized input.\n",
    "        x = x + self.attention(hidden_state)\n",
    "\n",
    "        # Same as before, but with the FFN layer instead of the MHA\n",
    "        # one.\n",
    "        # Note: we could have avoided defining the additional `hidden_state`\n",
    "        #       variable above by doing as we are doing here.\n",
    "        x = x + self.feed_forward(self.layer_norm_2(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b76568a-f35a-415b-aec1-da0c53f80c07",
   "metadata": {},
   "source": [
    "Test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5835c3d-dffa-4d81-bc09-ccc32925887b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layer = TransformerEncoderLayer(config=config)\n",
    "\n",
    "encoder_layer(test_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
