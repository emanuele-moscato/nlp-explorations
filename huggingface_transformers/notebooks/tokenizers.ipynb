{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18bd1732-fd6d-49c8-b08b-5b93573804df",
   "metadata": {},
   "source": [
    "# Huggingface tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73db57fc-e505-48d1-903c-49204d45170b",
   "metadata": {},
   "source": [
    "Let's test some of the tokenizers that Huggingface offers off-the-shelf. They are part of (i.e. the first layers of) pretrained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc70ee4-ecc4-45cc-bbbd-9c2db5332b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462bb8fe-6f44-4c28-88d9-3dd32fa8bd6f",
   "metadata": {},
   "source": [
    "Load a tokenizer from a pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbebb01-70b4-4703-b6ee-602ec76d2a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model checkpoint. Tells the AutoTokenizer object\n",
    "# which model (architecture and weight values) to\n",
    "# load.\n",
    "model_ckpt = 'distilbert-base-uncased'\n",
    "\n",
    "# Loads the tokenizer from a pretrained model.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638e3c9c-a8df-4bbc-aeb7-78e7290e70c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative syntax to load a specific class. This\n",
    "# specific one is the same as the above one.\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "distilbert_tokenizer = DistilBertTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc42608-9710-4ef8-b3f0-e76fac2c16fd",
   "metadata": {},
   "source": [
    "Tokenize sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d77fc6-b8f0-4395-afe7-de3589b8674d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Master Splinter wasn't happy at all with the\n",
    "training of the Turtles. Too ruthless, too open\n",
    "to taking risks. Maybe too young?\n",
    "\"\"\"\n",
    "\n",
    "encoded_text = tokenizer(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2110fa4-f3d6-4ba5-bead-dac6a7f13aae",
   "metadata": {},
   "source": [
    "The output of the tokenizer contains two parts: the IDs of the tokens (in the internal vocabulary of the tokenizer) and the `attention_mask`, which has the same length as the list of input IDs and contains 1 for each proper character and 0 for each padding character (if any - this of course doesn't show up when tokenizing a single sequence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba958c3-fe2b-400b-badf-268471065d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089e5fca-92b7-44db-8eae-6b4cf33db939",
   "metadata": {},
   "source": [
    "Convert the IDs back to tokens.\n",
    "\n",
    "**Note:** tokens like `[CLS]` and `[SEP]` are special tokens inserted by the tokenizers to signify something specific. In this case they respectively correspond to the start and end of the sequence (other special tokens exist, e.g. for padding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c478cf-0660-4d4c-8174-423becd37375",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(encoded_text['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3661ae22-dd9c-46f9-9c0e-0874f18e2c5e",
   "metadata": {},
   "source": [
    "Convert tokens back to the original string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53a2626-1c0b-473d-a76d-6a880958aecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_tokens_to_string(\n",
    "    tokenizer.convert_ids_to_tokens(encoded_text['input_ids'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7290ab-59d4-4f16-a316-5392f2a5e969",
   "metadata": {},
   "source": [
    "Maximum length of a sequence the tokenizer can tokenize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9926f22-ce8c-4c9d-8139-ce32d50b76f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a1ce06-ccba-4985-a969-7e441edae5b2",
   "metadata": {},
   "source": [
    "Tokenize more than one sentence at the same time. with the option `padding=True` all sentences tokenized as equal-length sequences of tokens as long as the longest one. Shorter sequences are made longer by introducing the padding token. In this case all `attention_mask`s have equal length and have component value 1 for legit tokens and 0 for padding tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8ffecd-7bc2-416c-ace7-9511397ad6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_2 = \"\"\"\n",
    "Not that the Turtles really cared about him worrying...\n",
    "\"\"\"\n",
    "\n",
    "tokenizer([text_2, text], padding=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
