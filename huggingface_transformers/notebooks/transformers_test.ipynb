{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "588e1b3b-f2e6-4672-b09a-ec0efc70813f",
   "metadata": {},
   "source": [
    "# Test Huggingface transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8994af78-09d1-4958-9441-0b603f646669",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, TRANSFORMERS_CACHE\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30951bef-12ac-423e-821f-64307ffbd4b5",
   "metadata": {},
   "source": [
    "## Downloading models and managing them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b043b8-8c10-4adc-8a18-f18bac40c3c7",
   "metadata": {},
   "source": [
    "Every time `pipeline` is asked to produce a model, if it's not already present locally it is downloaded to a local directory called `TRANSFORMERS_CACHE`. Downloaded models can be listed and deleted with the Transformers CLI (`pip install 'huggingface-hub[cli]'` and then `huggingface-cli delete-cache`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ef7241-bef9-4854-93bc-57d22b14ad95",
   "metadata": {},
   "source": [
    "## Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6770e7d9-2587-41ea-adac-f81b4d003517",
   "metadata": {},
   "source": [
    "Standard task for the `text-classification` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c1e55f-973c-4a72-8dbf-f8c5a6d797ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An abstraction around models and NLP tasks.\n",
    "classifier = pipeline('text-classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09706f54-a052-4f9e-8a1a-cd935cc72953",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_unacceptable = \"\"\"\n",
    "Not sure who to send this to, but I live in New York and I keep seeing four turtle-shaped\n",
    "individuals entering the sewers from a manhole that I can clearly\n",
    "see from my back window... sometimes accompanied by a huge rat!\n",
    "This is not acceptable!\n",
    "\"\"\"\n",
    "\n",
    "text_awesome = \"\"\"\n",
    "Not sure who to send this to, but I live in New York and I keep seeing four turtle-shaped\n",
    "individuals entering the sewers from a manhole that I can clearly\n",
    "see from my back window... sometimes accompanied by a huge rat!\n",
    "This is awesome!\n",
    "\"\"\"\n",
    "\n",
    "for text in [text_unacceptable, text_awesome]:\n",
    "    print('Text:')\n",
    "    print(text)\n",
    "    print('Model ouptut:', classifier(text))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d73a05-b65a-4f07-b89e-ab6ed3cc3079",
   "metadata": {},
   "source": [
    "\n",
    "## Named entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115d22e7-a511-432b-a475-72a9d38daefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The aggregation_strategy sets how words are grouped together\n",
    "# to form single entities.\n",
    "ner_tagger = pipeline('ner', aggregation_strategy='simple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bff279b-4e97-42a6-8ec5-42dc0a6cf3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_text = \"\"\"\n",
    "I would like to notify you that the individual\n",
    "Mr. Krang repeatedly tried to kidnap Dr. Baxter\n",
    "Stockman from his house in Newark in order to\n",
    "steal the blueprints for his particle accelerator,\n",
    "which he intended to mount on his Harley Davidson\n",
    "motorcycle.\n",
    "\"\"\"\n",
    "\n",
    "pd.DataFrame(ner_tagger(ner_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d25c6c-c118-4572-b275-ee945e6939e3",
   "metadata": {},
   "source": [
    "## Question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e22d112-c8ee-4240-95a0-f401449e5631",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = pipeline('question-answering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d189e3-b81c-41dd-b76d-95ca00d94ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'What does Krang want from Dr. Stockman?'\n",
    "\n",
    "reader(question=question, context=ner_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd6ee28-9f69-42be-8614-3088ea81f67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'Why does Krang want a particle accelerator from Dr. Stockman?'\n",
    "\n",
    "reader(question=question, context=ner_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa0a9fd-4469-41c0-bd8c-8247c8721a23",
   "metadata": {},
   "source": [
    "## Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d476a4e9-3f75-4e49-8a18-be578dd6d021",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline('summarization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3299a97b-5398-4b02-9888-fecb8b58f321",
   "metadata": {},
   "outputs": [],
   "source": [
    "summ_text = \"\"\"\n",
    "This book is the fruit of several decades of reading, teaching, and thinking about the intersections between literature, philosophy, and physics. Obviously that trajectory encompassed so many more writers, thinkers, and scientists than these three. So when the time came to wrangle the project into a book, the question became: How to organize it? Who are the best characters through whom to tell this story, and how many should there be?\n",
    "\n",
    "My first stabs at a structure were more expansive. I liked the idea of telling stories about specific human beings and distilling their insights out of those stories. But at first there were simply too many stories there — I believe I outlined a 12-chapter book with a different central character for each chapter — and the book felt scattered, even if the core intellectual project was the same. After that I reigned it in, but perhaps a little too much.\n",
    "\n",
    "I sketched out what would have been a literary biography of one man. It was Boethius, believe it or not, who still plays a minor role in one of the chapters. But Boethius was too far away historically from some of the major innovations of 20th-century physics that I wanted to engage with. Then it hit me. Some years ago, I had published a little article on what would become the topic of the book in The New York Times. It had three central characters: [Jorge Luis] Borges, [Werner] Heisenberg, and [Immanuel] Kant. They had been there all the time! Returning to those specific figures I realized that among them, they had all the elements I needed.\n",
    "\n",
    "The core idea was always to show how thinking deeply about a problem can lead to profound insights independently of the specific field of the thinker. In other words, “soft” humanistic approaches can enlighten “hard” scientific ones, and vice versa. In these three I believed I had found my proof of concept, since reading Borges, and then using Kant to think through some of the questions provoked by Borges, had over the years led me to a deeper understanding of what Heisenberg had actually discovered than had just reading Heisenberg and explanations of his discovery.\n",
    "\n",
    "To be a bit more specific, Borges’ story about a man who becomes incapable of forgetting anything, incapable of any slippages or gaps in his perception of the world, when read through Kant’s analysis of the synthesis required for any experience in time and space to take place in the first place, lay bare in clarion (albeit non-mathematical) logic what Heisenberg had proven in his 1927 paper: An observation, say of a particle in motion, can’t ever achieve perfection because the very essence of an observation depends on there being a minimal difference between what is observed and what is observing.\n",
    "\"\"\"\n",
    "\n",
    "summary = summarizer(summ_text, max_length=100, clean_up_tokenization_spaces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9cc147-7413-4c6f-9ce9-d9919da9e1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7de473a-3c3c-481c-8765-0847bab2e416",
   "metadata": {},
   "source": [
    "## Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0b3c5d-b07f-457a-ae0d-d639011765a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = pipeline('translation_en_to_fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab4ed12-44d3-4ac2-ba67-435a376aea0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation = translator(\n",
    "    ner_text,\n",
    "    clean_up_tokenization_spaces=True,\n",
    "    min_length=100\n",
    ")\n",
    "\n",
    "translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2d73a5-2e48-41f0-8ad8-b9cf8db3f485",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation = translator(\n",
    "    summ_text[:600],\n",
    "    clean_up_tokenization_spaces=True,\n",
    "    min_length=100\n",
    ")\n",
    "\n",
    "translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f537abb-95ab-49f0-9fbd-4bd5fc8e6022",
   "metadata": {},
   "source": [
    "## Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fa2674-79d5-4cc9-b4d2-f349618f40d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline('text-generation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3363f2bd-e387-4846-98ce-ef71a89b5e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = 'Good evening sir, this is the police, we just received your message.'\n",
    "prompt = ner_text + '\\n\\nAnswer from the police:\\n' + response\n",
    "\n",
    "generated_reponse = generator(prompt, max_length=200)\n",
    "\n",
    "print(generated_reponse)print(generated_reponse[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
